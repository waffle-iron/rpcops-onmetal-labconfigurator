#!/bin/bash

# Check for existing lab
if [ -e "/root/.labconfig" ]; then
  echo -e "Lab exists already"
  cat /root/.labconfig
  exit 1
fi

# Configure cobbler (unattended setup handler)

# $1 will be the lab configuration requested
# if not set, will use default
# ex. bash rpcops-lab-setup cephonly

# Get all defined lab configurations
# LABS=( `ls -1 resources/labconfigs/` )
#
# # Check if a configuration was passed in [default] if not
# if [ "$#" -eq 1 ]; then
#   for LAB in ${LABS[@]}
#   do
#     # Set LAB_CONFIG
#     if [ $LAB == $1 ]; then
#       LAB_CONFIG=$1
#     else
#       LAB_CONFIG='default'
#     fi
#   done
# else
#   LAB_CONFIG='default'
# fi

# Move to the root directory as starting point
cd /root/rpcops-onmetal-labconfigurator

# Set LAB_CONFIG [default for now]
if ! [ -z "$1" ]; then
  LAB_CONFIG=$1
else
  LAB_CONFIG='default'
fi

# Set BACKING_FILE
BACKING_FILE='/var/lib/libvirt/images/nodebase.qcow2'

# This gets the root users SSH-public-key
SSHKEY=$(cat /root/.ssh/id_rsa.pub|cut -d' ' -f1,2)

# Node types
TYPES=( infra logging compute object storage ceph )

function wait_ssh() {
echo "Waiting for all nodes to become available."
for node in $(get_all_hosts); do
    echo "Waiting for node: ${node%%":"*} on 172.29.236.${node#*":"}"
    ssh -q -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 172.29.236.${node#*":"} exit > /dev/null
    while test $? -gt 0; do
      sleep 7
      ssh -q -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 172.29.236.${node#*":"} exit > /dev/null
    done
done
}

function get_host_type () {
python <<EOF
import json
with open('resources/labconfigs/${LAB_CONFIG}.json') as f:
    x = json.loads(f.read())
    try:
        for k, v in x.get("$1").items():
            print('%s:%s' % (k, v))
    except:
        pass
EOF
}

function get_all_hosts () {
python <<EOF
import json
with open('resources/labconfigs/${LAB_CONFIG}.json') as f:
    x = json.loads(f.read())
for i in x.values():
    for k, v in i.items():
      print('%s:%s' % (k, v))
EOF
}

function check_for_pool () {
  virsh pool-list|grep -q storage
  rc=$?
  if ! [ $rc -eq 0 ]; then
    virsh pool-define-as --name storage --type dir --target /var/lib/libvirt/images
    virsh pool-autostart --pool storage
    virsh pool-start --pool storage
    virsh pool-refresh --pool storage
  fi
}

# Create the VM root disk then define and start the VMs.
#  !!!THIS TASK WILL DESTROY ALL OF THE ROOT DISKS IF THEY ALREADY EXIST!!!
for node in $(get_all_hosts); do
  # create node storage using backing image
  check_for_pool
  virsh vol-create-as storage ${node%%":"*}.qcow2 256000M --backing-vol ${BACKING_FILE} --backing-vol-format qcow2 --format qcow2
  #qemu-img create -f qcow2 -b ${BACKING_FILE} /var/lib/libvirt/images/${node%%":"*}.qcow2
  # create node xml file
  cp resources/nodes/node.xml /etc/libvirt/qemu/${node%%":"*}.xml
  sed -i "s/__NODE__/${node%%":"*}/g" /etc/libvirt/qemu/${node%%":"*}.xml
  # adjust ram and cpu if compute node
  if [[ ${node%%":"*} =~ "compute" ]]; then
    sed -i "s/__RAM__/16777216/g" /etc/libvirt/qemu/${node%%":"*}.xml
    sed -i "s/__CPU__/16/" /etc/libvirt/qemu/${node%%":"*}.xml
  else
    sed -i "s/__RAM__/8388608/g" /etc/libvirt/qemu/${node%%":"*}.xml
    sed -i "s/__CPU__/8/" /etc/libvirt/qemu/${node%%":"*}.xml
  fi
  # define node
  virsh define /etc/libvirt/qemu/${node%%":"*}.xml || true

  # Change to storage pool directory
  pushd /var/lib/libvirt/images

  # update /etc/hosts
  # update /etc/network/interfaces
  # update /etc/network/interfaces.d/node.cfg
  # update /etc/hostname
  echo -e "Updating /etc/hosts on node: ${node%%":"*}\n"
  echo -e "Updating /etc/network/interfaces of node: ${node%%":"*}\n"
  echo -e "Updating /etc/network/interfaces.d/node.cfg of node: ${node%%":"*}\n"
  echo -e "Updating hostname of node: ${node%%":"*}\n"
  guestfish -d ${node%%":"*} -i <<EOG
command "sed -i -e 's/nodebase/${node%%":"*}/g' -e 's/10.5.0.5/10.5.0.${node:(-3)}/' /etc/hosts"
sh 'sed -i -e "s/nodebase/${node%%":"*}/g" -e "s/10.5.0.5/10.5.0.${node:(-3)}/" /etc/network/interfaces'
sh 'sed -i -re "s/nodebase/${node%%":"*}/g" /etc/network/interfaces.d/node.cfg'
sh 'sed -i -re "s/(([0-9]{1,3}[.]){3})(5)/\1${node:(-3)}/g" /etc/network/interfaces.d/node.cfg'
sh 'echo ${node%%":"*} > /etc/hostname'
EOG
  popd
done

for node in $(get_host_type compute); do
  echo -e "Creating and configuring nova logical volume for ${node%%":"*}"
  guestfish -d ${node%%":"*} -i <<EOG
lvcreate-free nova lxc 100
mkfs ext3 /dev/lxc/nova
mkdir-p /var/lib/nova
mount /dev/lxc/nova /var/lib/nova
write-append /etc/fstab '/dev/mapper/lxc-nova    /var/lib/nova    ext3    rw    0    0'
EOG
done

for node in $(get_host_type storage); do
  echo -e "Creating block storage block device for ${node%%":"*}"
  virsh vol-create-as storage ${node%%":"*}-cv-disk 102400M --format qcow2 --prealloc-metadata
  #qemu-img create -f raw /var/lib/libvirt/images/${node%%":"*}-cv-disk 102400M
  #virsh attach-disk ${node%%":"*} --source /var/lib/libvirt/images/${node%%":"*}-cv-disk --target vdb --persistent
done

for node in $(get_host_type object); do
  echo -e "Creating object storage block devices for ${node%%":"*}"
  for disk in {b..f}; do
    virsh vol-create-as storage ${node%%":"*}-ob-${disk} 20480M --format qcow2 --prealloc-metadata
    #qemu-img create -f raw /var/lib/libvirt/images/${node%%":"*}-ob-${disk} 20480M
    #virsh attach-disk ${node%%":"*} --source /var/lib/libvirt/images/${node%%":"*}-ob-${disk} --target vd${disk} --persistent
  done
done

# Check if infra01 exists, if not, skip infra01 specific tasks
virsh list --all --name|grep infra01
rc=$?
if [ $rc -eq 0 ]; then
  echo -e "Generating infra01 hosts file"
  for node in $(get_all_hosts); do
    echo -e "172.29.236.${node:(-3)}\t${node%%":"*}" >> /root/rpcops-onmetal-labconfigurator/hosts
  done
  echo -e "Adding hosts file to infra01"
  virt-copy-in -d infra01 /root/rpcops-onmetal-labconfigurator/hosts /etc
  rm -f /root/rpcops-onmetal-labconfigurator/hosts

  echo -e "Adding jq helper library to infra01"
  virt-copy-in -d infra01 /root/rpcops-onmetal-labconfigurator/resources/files/jq /usr/local/bin

  echo -e "Adding VPX configuration script to infra01"
  virt-copy-in -d infra01 /root/rpcops-onmetal-labconfigurator/resources/files/vpx-configurator /root

  echo -e "Adding VPX cleaner script to infra01"
  virt-copy-in -d infra01 /root/rpcops-onmetal-labconfigurator/resources/files/vpx-cleaner /root
fi

for node in $(get_all_hosts); do
# create node
  echo -e "Creating and starting node: ${node%%":"*}\n"
  virsh create /etc/libvirt/qemu/${node%%":"*}.xml
done

# Wait here for all nodes to be booted and ready with SSH
wait_ssh

echo -e "Configuration succeeded for lab: ${LAB_CONFIG}\n"
echo -e "Login for all nodes user: root | password: stack\n"
for node in $(get_all_hosts); do
  echo -e "Node Name: ${node%%":"*}\t\tDRAC IP: 172.29.236.${node:(-3)}\n"
done

echo -e "Updating /etc/hosts with lab nodes\n"
for node in $(get_all_hosts); do
  echo -e "172.29.236.${node:(-3)}\t${node%%":"*}" >> /etc/hosts
done

echo -e "Creating inventory file\n"
for type in "${TYPES[@]}"; do
  echo -e "[${type}]" >> /root/rpcops-onmetal-labconfigurator/inventory
  for node in $(get_host_type ${type}); do
    echo -e "${node%%":"*}" >> /root/rpcops-onmetal-labconfigurator/inventory
  done
  echo -ne "\n"
done

echo -e "Clean known_hosts"
: > /root/.ssh/known_hosts

for node in $(get_host_type storage); do
  echo -e "Attaching block storage block device for ${node%%":"*}"
  virsh attach-disk --domain ${node%%":"*} --source /var/lib/libvirt/images/${node%%":"*}-cv-disk --target vdb --persistent --driver qemu --sourcetype file --subdriver qcow2
done

for node in $(get_host_type object); do
  echo -e "Attaching object storage block devices for ${node%%":"*}"
  for disk in {b..f}; do
    virsh attach-disk --domain ${node%%":"*} --source /var/lib/libvirt/images/${node%%":"*}-ob-${disk} --target vd${disk} --persistent --driver qemu --sourcetype file --subdriver qcow2
  done
done

echo -e "Create \"Mint Condition\" snapshots"
for node in $(get_all_hosts); do
  virsh snapshot-create-as --domain ${node%%":"*} --name ${node%%":"*}-mint --atomic --description 'Mint Condition'
done

echo -e "Storing lab configuration used"
echo ${LAB_CONFIG} > /root/.labconfig
